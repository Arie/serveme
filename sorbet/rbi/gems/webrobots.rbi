# This file is autogenerated. Do not edit it by hand. Regenerate it with:
#   srb rbi gems

# typed: true
#
# If you would like to make changes to this file, great! Please create the gem's shim here:
#
#   https://github.com/sorbet/sorbet-typed/new/master?filename=lib/webrobots/all/webrobots.rbi
#
# webrobots-0.1.2

module Webrobots
end
class WebRobots
  def allowed?(url); end
  def crawl_delay(url); end
  def crawl_delay_handler(delay, last_checked_at); end
  def create_cache; end
  def disallowed?(url); end
  def error!(url); end
  def error(url); end
  def fetch_robots_txt(site); end
  def flush_cache; end
  def get_robots_txt(site); end
  def http_get(uri); end
  def initialize(user_agent, options = nil); end
  def option(url, token); end
  def options(url); end
  def reset(url); end
  def robots_txt_for(url); end
  def sitemaps(url); end
  def split_uri(url); end
  def user_agent; end
end
class WebRobots::Error < StandardError
end
class WebRobots::ParseError < WebRobots::Error
  def initialize(message, site); end
  def site; end
  def to_s; end
end
class WebRobots::RobotsTxt
  def allow?(request_uri, user_agent = nil); end
  def crawl_delay(user_agent = nil); end
  def error!; end
  def error; end
  def error=(arg0); end
  def find_record(user_agent = nil); end
  def initialize(site, records, options = nil); end
  def options(user_agent = nil); end
  def self.unfetchable(site, reason, target = nil); end
  def site; end
  def sitemaps; end
  def target(user_agent = nil); end
  def timestamp; end
end
class WebRobots::RobotsTxt::Parser < Racc::Parser
  def _reduce_1(val, _values, result); end
  def _reduce_17(val, _values, result); end
  def _reduce_18(val, _values, result); end
  def _reduce_19(val, _values, result); end
  def _reduce_2(val, _values, result); end
  def _reduce_20(val, _values, result); end
  def _reduce_21(val, _values, result); end
  def _reduce_24(val, _values, result); end
  def _reduce_25(val, _values, result); end
  def _reduce_26(val, _values, result); end
  def _reduce_28(val, _values, result); end
  def _reduce_31(val, _values, result); end
  def _reduce_32(val, _values, result); end
  def _reduce_38(val, _values, result); end
  def _reduce_39(val, _values, result); end
  def _reduce_40(val, _values, result); end
  def _reduce_41(val, _values, result); end
  def _reduce_none(val, _values, result); end
  def initialize(target, crawl_delay_handler = nil); end
  def next_token; end
  def on_error(token_id, value, stack); end
  def parse!(input, site); end
  def parse(input, site); end
  def parse_error(message); end
end
class WebRobots::RobotsTxt::Record
  def allow?(request_uri); end
  def default?; end
  def delay; end
  def initialize(agentlines, rulelines); end
  def match?(user_agent); end
  def options; end
end
class WebRobots::RobotsTxt::Line
  def compile; end
  def initialize(token, value); end
  def token; end
  def value; end
end
class WebRobots::RobotsTxt::AgentLine < WebRobots::RobotsTxt::Line
  def compile; end
  def pattern; end
end
class WebRobots::RobotsTxt::AccessControlLine < WebRobots::RobotsTxt::Line
  def compile; end
  def match?(request_uri); end
end
class WebRobots::RobotsTxt::AllowLine < WebRobots::RobotsTxt::AccessControlLine
  def allow?; end
end
class WebRobots::RobotsTxt::DisallowLine < WebRobots::RobotsTxt::AccessControlLine
  def allow?; end
end
class WebRobots::RobotsTxt::CrawlDelayLine < WebRobots::RobotsTxt::Line
  def compile; end
  def delay; end
end
class WebRobots::RobotsTxt::ExtentionLine < WebRobots::RobotsTxt::Line
end
class Nokogiri::HTML4::Document < Nokogiri::XML::Document
  def meta_robots(custom_name = nil); end
  def nofollow?(custom_name = nil); end
  def noindex?(custom_name = nil); end
  def parse_meta_robots(custom_name); end
end
